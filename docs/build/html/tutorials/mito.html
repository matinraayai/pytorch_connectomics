

<!DOCTYPE html>
<html class="writer-html4" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Mitochondria Segmentation &mdash; connectomics latest documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'../',
              VERSION:'latest',
              LANGUAGE:'None',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Synaptic Cleft Segmentation" href="cremi.html" />
    <link rel="prev" title="Neuron Segmentation" href="snemi.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/logo_text.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                latest
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notes/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/config.html">Configurations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../notes/config.html#basic-usage">Basic Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes/config.html#multiple-losses-for-a-single-learning-target">Multiple Losses for a Single Learning Target</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes/config.html#multitask-learning">Multitask Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes/config.html#inference">Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../notes/dataloading.html">Data Loading</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../notes/dataloading.html#data-augmentation">Data Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes/dataloading.html#rejection-sampling">Rejection Sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes/dataloading.html#tiledataset">TileDataset</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="snemi.html">Neuron Segmentation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Mitochondria Segmentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#semantic-segmentation">Semantic Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#instance-segmentation">Instance Segmentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cremi.html">Synaptic Cleft Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="synaptic_partner.html">Synaptic Partner Segmentation</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules/datasets.html">connectomics.data.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/augmentation.html">connectomics.data.augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/engine.html">connectomics.engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/model.html">connectomics.model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../modules/model.html#module-connectomics.model.block">Building Blocks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../modules/model.html#module-connectomics.model.zoo">Model Zoo</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../modules/utils.html">connectomics.utils</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../modules/utils.html#module-connectomics.utils.processing">Post-processing</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">connectomics</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Mitochondria Segmentation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorials/mito.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="mitochondria-segmentation">
<h1>Mitochondria Segmentation<a class="headerlink" href="#mitochondria-segmentation" title="Permalink to this headline">¶</a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#introduction" id="id4">Introduction</a></li>
<li><a class="reference internal" href="#semantic-segmentation" id="id5">Semantic Segmentation</a></li>
<li><a class="reference internal" href="#instance-segmentation" id="id6">Instance Segmentation</a></li>
</ul>
</div>
<div class="section" id="introduction">
<h2><a class="toc-backref" href="#id4">Introduction</a><a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Mitochondrion">Mitochondria</a> are the primary energy providers for cell activities, thus essential for metabolism.
Quantification of the size and geometry of mitochondria is not only crucial to basic neuroscience research, but also informative to
clinical studies including, but not limited to, bipolar disorder and diabetes.</p>
<p>This tutorial has two parts. In the first part, you will learn how to make <strong>pixel-wise class prediction</strong> on the widely used benchmark
dataset released by <a class="reference external" href="https://ieeexplore.ieee.org/document/6619103">Lucchi et al.</a> in 2012. In the second part, you will learn how to predict the <strong>instance masks</strong> of
individual mitochondrion from the large-scale MitoEM dataset released by <a class="reference external" href="https://donglaiw.github.io/paper/2020_miccai_mitoEM.pdf">Wei et al.</a> in 2020.</p>
</div>
<div class="section" id="semantic-segmentation">
<h2><a class="toc-backref" href="#id5">Semantic Segmentation</a><a class="headerlink" href="#semantic-segmentation" title="Permalink to this headline">¶</a></h2>
<p>This section provides step-by-step guidance for mitochondria segmentation with the EM benchmark datasets released by <a class="reference external" href="https://cvlab.epfl.ch/research/page-90578-en-html/research-medical-em-mitochondria-index-php/">Lucchi et al.</a>.
We consider the task as a <strong>semantic segmentation</strong> task and predict the mitochondria pixels with encoder-decoder ConvNets similar to
the models used in affinity prediction in <a class="reference external" href="https://zudi-lin.github.io/pytorch_connectomics/build/html/tutorials/snemi.html">neuron segmentation</a>. The evaluation of the mitochondria segmentation results is based on the F1 score and Intersection over Union (IoU).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Different from other EM connectomics datasets used in the tutorials, the dataset released by Lucchi et al. is an isotropic dataset,
which means the spatial resolution along all three axes is the same. Therefore a completely 3D U-Net and data augmentation along z-x
and z-y planes besides x-y planes are preferred.</p>
</div>
<p>All the scripts needed for this tutorial can be found at <code class="docutils literal"><span class="pre">pytorch_connectomics/scripts/</span></code>. Need to pass the argument <code class="docutils literal"><span class="pre">--config-file</span> <span class="pre">configs/Lucchi-Mitochondria.yaml</span></code> during training and inference to load the required configurations for this task.
The pytorch dataset class of lucchi data is <a class="reference internal" href="../modules/datasets.html#connectomics.data.dataset.VolumeDataset" title="connectomics.data.dataset.VolumeDataset"><code class="xref py py-class docutils literal"><span class="pre">connectomics.data.dataset.VolumeDataset</span></code></a>.</p>
<div class="figure align-center" id="id2">
<a class="reference internal image-reference" href="../_images/lucchi_qual.png"><img alt="../_images/lucchi_qual.png" src="../_images/lucchi_qual.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-text">Qualitative results of the model prediction on the mitochondria segmentation dataset released by
Lucchi et al., without any post-processing.</span></p>
</div>
<ol class="arabic">
<li><p class="first">Get the dataset:</p>
<blockquote>
<div><p>Download the dataset from our server:</p>
<blockquote>
<div><div class="highlight-none"><div class="highlight"><pre><span></span>wget http://rhoana.rc.fas.harvard.edu/dataset/lucchi.zip
</pre></div>
</div>
</div></blockquote>
<p>For description of the data please check <a class="reference external" href="https://www.epfl.ch/labs/cvlab/data/data-em/">the author page</a>.</p>
</div></blockquote>
</li>
<li><p class="first">Run the training script:</p>
<blockquote>
<div><div class="highlight-none"><div class="highlight"><pre><span></span>$ source activate py3_torch
$ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u scripts/main.py \
  --config-file configs/Lucchi-Mitochondria.yaml
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">Visualize the training progress:</p>
<blockquote>
<div><div class="highlight-none"><div class="highlight"><pre><span></span>$ tensorboard --logdir runs
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">Run inference on test image volume:</p>
<blockquote>
<div><div class="highlight-none"><div class="highlight"><pre><span></span>$ source activate py3_torch
$ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u scripts/main.py \
  --config-file configs/Lucchi-Mitochondria.yaml --inference \
  --checkpoint outputs/Lucchi_mito_baseline/volume_100000.pth.tar
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">Since the ground-truth label of the test set is public, we can run the evaluation locally:</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">connectomics.utils.evaluation</span> <span class="kn">import</span> <span class="n">get_binary_jaccard</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span> <span class="o">/</span> <span class="mf">255.</span> <span class="c1"># output is casted to uint8 with range [0,255].</span>
<span class="n">gt</span> <span class="o">=</span> <span class="p">(</span><span class="n">gt</span><span class="o">!==</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">thres</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]</span> <span class="c1"># evaluate at multiple thresholds.</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">get_binary_jaccard</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">gt</span><span class="p">,</span> <span class="n">thres</span><span class="p">)</span>
</pre></div>
</div>
<p>The prediction can be further improved by conducting median filtering to remove noise:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">connectomics.utils.evaluation</span> <span class="kn">import</span> <span class="n">get_binary_jaccard</span>
<span class="kn">from</span> <span class="nn">connectomics.utils.processing</span> <span class="kn">import</span> <span class="n">binarize_and_median</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span> <span class="o">/</span> <span class="mf">255.</span> <span class="c1"># output is casted to uint8 with range [0,255].</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">binarize_and_median</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="n">thres</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">gt</span> <span class="o">=</span> <span class="p">(</span><span class="n">gt</span><span class="o">!==</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">get_binary_jaccard</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span> <span class="c1"># prediction is already binarized</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
<p>Our pretained model achieves a foreground IoU and IoU of <strong>0.892</strong> and <strong>0.943</strong> on the test set, respectively. The results are better or on par with
state-of-the-art approaches. Please check <a class="reference external" href="https://github.com/zudi-lin/pytorch_connectomics/blob/master/BENCHMARK.md">BENCHMARK.md</a>  for detailed performance
comparison and the pre-trained models.</p>
</div>
<div class="section" id="instance-segmentation">
<h2><a class="toc-backref" href="#id6">Instance Segmentation</a><a class="headerlink" href="#instance-segmentation" title="Permalink to this headline">¶</a></h2>
<p>This section provides step-by-step guidance for mitochondria segmentation with our benchmark datasets <a class="reference external" href="https://donglaiw.github.io/page/mitoEM/index.html">MitoEM</a>.
We consider the task as 3D <strong>instance segmentation</strong> task and provide three different confiurations of the model output.
The model is <code class="docutils literal"><span class="pre">unet_res_3d</span></code>, similar to the one used in <a class="reference external" href="https://zudi-lin.github.io/pytorch_connectomics/build/html/tutorials/snemi.html">neuron segmentation</a>.
The evaluation of the segmentation results is based on the AP-75 (average precision with an IoU threshold of 0.75).</p>
<div class="figure align-center" id="id3">
<a class="reference internal image-reference" href="../_images/mito_complex.png"><img alt="../_images/mito_complex.png" src="../_images/mito_complex.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-text">Complex mitochondria in the MitoEM dataset:(a) mitochondria-on-a-string (MOAS), and (b) dense tangle of touching instances.
Those challenging cases are prevalent but not covered in previous datasets.</span></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The MitoEM dataset has two sub-datasets <strong>Rat</strong> and <strong>Human</strong> based on the source of the tissues. Three training configuration files on <strong>MitoEM-Rat</strong>
are provided in <code class="docutils literal"><span class="pre">pytorch_connectomics/configs/MitoEM/</span></code> for different learning targets of the model.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Since the dataset is very large and can not be directly loaded into memory, we use the <a class="reference internal" href="../modules/datasets.html#connectomics.data.dataset.TileDataset" title="connectomics.data.dataset.TileDataset"><code class="xref py py-class docutils literal"><span class="pre">connectomics.data.dataset.TileDataset</span></code></a> dataset class that only
loads part of the whole volume by opening involved <code class="docutils literal"><span class="pre">PNG</span></code> images.</p>
</div>
<ol class="arabic">
<li><p class="first">Introduction to the dataset:</p>
<blockquote>
<div><p>On the Harvard RC cluster, the datasets can be found at:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>/n/pfister_lab2/Lab/vcg_connectomics/mitochondria/miccai2020/rat
</pre></div>
</div>
<p>and</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>/n/pfister_lab2/Lab/vcg_connectomics/mitochondria/miccai2020/human
</pre></div>
</div>
<p>For the public link of the dataset, check the <a class="reference external" href="https://donglaiw.github.io/page/mitoEM/index.html">project page</a>.</p>
<p>Dataset description:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">im</span></code>: includes 1,000 single-channel <code class="docutils literal"><span class="pre">*.png</span></code> files (<strong>4096x4096</strong>) of raw EM images (with a spatial resolution of <strong>30x8x8</strong> nm).</li>
<li><code class="docutils literal"><span class="pre">mito</span></code>: includes 1,000 single-channel <code class="docutils literal"><span class="pre">*.png</span></code> files (<strong>4096x4096</strong>) of instance labels.</li>
<li><code class="docutils literal"><span class="pre">*.json</span></code>: <code class="xref py py-class docutils literal"><span class="pre">Dict</span></code> contains paths to <code class="docutils literal"><span class="pre">*.png</span></code> files</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Configure <code class="docutils literal"><span class="pre">.yaml</span></code> files for different learning targets.</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">MitoEM-R-A.yaml</span></code>: output 3 channels for affinty prediction.</li>
<li><code class="docutils literal"><span class="pre">MitoEM-R-AC.yaml</span></code>: output 4 channels for both affinity and instance contour prediction.</li>
<li><code class="docutils literal"><span class="pre">MitoEM-R-BC.yaml</span></code>: output 2 channels for both binary mask and instance contour prediction.</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Run the training script.</p>
<blockquote>
<div><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">By default the path of images and labels are not specified. To
run the training scripts, please revise the <code class="docutils literal"><span class="pre">DATASET.IMAGE_NAME</span></code>, <code class="docutils literal"><span class="pre">DATASET.LABEL_NAME</span></code>, <code class="docutils literal"><span class="pre">DATASET.OUTPUT_PATH</span></code>
and <code class="docutils literal"><span class="pre">DATASET.INPUT_PATH</span></code> options in <code class="docutils literal"><span class="pre">configs/MitoEM-R-*.yaml</span></code>.
The options can also be given as command-line arguments without changing of the <code class="docutils literal"><span class="pre">yaml</span></code> configuration files.</p>
</div>
<div class="highlight-none"><div class="highlight"><pre><span></span>$ source activate py3_torch
$ python -u scripts/main.py --config-file configs/MitoEM-R-A.yaml
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">Visualize the training progress. More info <a class="reference external" href="https://vcg.github.io/newbie-wiki/build/html/computation/machine_rc.html">here</a>:</p>
<blockquote>
<div><div class="highlight-none"><div class="highlight"><pre><span></span>$ tensorboard --logdir ``OUTPUT_PATH/&lt;EXP_DIR_NAME&gt;``
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Our utility functions will create a subdir in OUTPUT_PATH to save the Tensorboard event files. Substitute <strong>&lt;EXP_DIR_NAME&gt;</strong> with your subdir name.</p>
</div>
</div></blockquote>
</li>
<li><p class="first">Run inference on image volumes:</p>
<blockquote>
<div><div class="highlight-none"><div class="highlight"><pre><span></span>$ source activate py3_torch
$ python -u scripts/main.py \
  --config-file configs/MitoEM-R-A.yaml --inference \
  --checkpoint OUTPUT_PATH/checkpoint_&lt;ITER_NUM&gt;.pth.tar
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Please change the <code class="docutils literal"><span class="pre">INFERENCE.IMAGE_NAME</span></code> <code class="docutils literal"><span class="pre">INFERENCE.OUTPUT_PATH</span></code> <code class="docutils literal"><span class="pre">INFERENCE.OUTPUT_NAME</span></code>
options in <code class="docutils literal"><span class="pre">configs/MitoEM-R-A.yaml</span></code>.</p>
</div>
</div></blockquote>
</li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="cremi.html" class="btn btn-neutral float-right" title="Synaptic Cleft Segmentation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="snemi.html" class="btn btn-neutral float-left" title="Neuron Segmentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Zudi Lin and Donglai Wei

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>